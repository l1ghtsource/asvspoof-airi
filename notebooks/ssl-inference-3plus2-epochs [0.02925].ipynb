{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c89622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T06:55:12.362112Z",
     "iopub.status.busy": "2024-11-27T06:55:12.361856Z",
     "iopub.status.idle": "2024-11-27T06:55:16.993875Z",
     "shell.execute_reply": "2024-11-27T06:55:16.992954Z"
    },
    "papermill": {
     "duration": 4.639183,
     "end_time": "2024-11-27T06:55:16.996051",
     "exception": false,
     "start_time": "2024-11-27T06:55:12.356868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SSL_Anti-spoofing'...\r\n",
      "remote: Enumerating objects: 1579, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (90/90), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (42/42), done.\u001b[K\r\n",
      "remote: Total 1579 (delta 52), reused 82 (delta 48), pack-reused 1489 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (1579/1579), 30.57 MiB | 18.20 MiB/s, done.\r\n",
      "Resolving deltas: 100% (293/293), done.\r\n"
     ]
    }
   ],
   "source": [
    "import git\n",
    "!git clone https://github.com/TakHemlata/SSL_Anti-spoofing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb479d3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T06:55:17.005824Z",
     "iopub.status.busy": "2024-11-27T06:55:17.005517Z",
     "iopub.status.idle": "2024-11-27T06:55:17.012110Z",
     "shell.execute_reply": "2024-11-27T06:55:17.011422Z"
    },
    "papermill": {
     "duration": 0.01321,
     "end_time": "2024-11-27T06:55:17.013702",
     "exception": false,
     "start_time": "2024-11-27T06:55:17.000492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files and folders moved from /kaggle/working/SSL_Anti-spoofing to /kaggle/working/\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "source_dir = '/kaggle/working/SSL_Anti-spoofing'\n",
    "destination_dir = \"/kaggle/working/\"\n",
    "\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "for item_name in os.listdir(source_dir):\n",
    "    source_item = os.path.join(source_dir, item_name)\n",
    "    destination_item = os.path.join(destination_dir, item_name)\n",
    "    \n",
    "    shutil.move(source_item, destination_item)\n",
    "\n",
    "print(f\"All files and folders moved from {source_dir} to {destination_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9209d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T06:55:17.022955Z",
     "iopub.status.busy": "2024-11-27T06:55:17.022713Z",
     "iopub.status.idle": "2024-11-27T06:55:17.027689Z",
     "shell.execute_reply": "2024-11-27T06:55:17.026950Z"
    },
    "papermill": {
     "duration": 0.011399,
     "end_time": "2024-11-27T06:55:17.029365",
     "exception": false,
     "start_time": "2024-11-27T06:55:17.017966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /kaggle/working/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/fairseq/data/indexed_dataset.py deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/fairseq/data/indexed_dataset.py\"\n",
    "\n",
    "try:\n",
    "    os.remove(file_path)\n",
    "    print(f\"File {file_path} deleted successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {file_path} does not exist.\")\n",
    "except PermissionError:\n",
    "    print(\"You do not have permission to delete this file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c391b0d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T06:55:17.039317Z",
     "iopub.status.busy": "2024-11-27T06:55:17.039080Z",
     "iopub.status.idle": "2024-11-27T06:55:17.047822Z",
     "shell.execute_reply": "2024-11-27T06:55:17.046977Z"
    },
    "papermill": {
     "duration": 0.015991,
     "end_time": "2024-11-27T06:55:17.049369",
     "exception": false,
     "start_time": "2024-11-27T06:55:17.033378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied /kaggle/input/float32/indexed_dataset.py to /kaggle/working/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/fairseq/data/indexed_dataset.py\n"
     ]
    }
   ],
   "source": [
    "source_path = \"/kaggle/input/float32/indexed_dataset.py\"\n",
    "destination_path = \"/kaggle/working/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/fairseq/data/indexed_dataset.py\"\n",
    "\n",
    "shutil.copy2(source_path, destination_path)\n",
    "\n",
    "print(f\"Copied {source_path} to {destination_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ee1c960",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T06:55:17.058688Z",
     "iopub.status.busy": "2024-11-27T06:55:17.058039Z",
     "iopub.status.idle": "2024-11-27T06:56:03.352107Z",
     "shell.execute_reply": "2024-11-27T06:56:03.351281Z"
    },
    "papermill": {
     "duration": 46.300707,
     "end_time": "2024-11-27T06:56:03.354107",
     "exception": false,
     "start_time": "2024-11-27T06:55:17.053400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///kaggle/working/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: cffi in /opt/conda/lib/python3.10/site-packages (from fairseq==1.0.0a0+4acaa61) (1.16.0)\r\n",
      "Requirement already satisfied: cython in /opt/conda/lib/python3.10/site-packages (from fairseq==1.0.0a0+4acaa61) (3.0.10)\r\n",
      "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==1.0.0a0+4acaa61)\r\n",
      "  Downloading hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Collecting omegaconf<2.1 (from fairseq==1.0.0a0+4acaa61)\r\n",
      "  Downloading omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fairseq==1.0.0a0+4acaa61) (1.26.4)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from fairseq==1.0.0a0+4acaa61) (2024.5.15)\r\n",
      "Collecting sacrebleu>=1.4.12 (from fairseq==1.0.0a0+4acaa61)\r\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from fairseq==1.0.0a0+4acaa61) (2.4.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fairseq==1.0.0a0+4acaa61) (4.66.4)\r\n",
      "Collecting bitarray (from fairseq==1.0.0a0+4acaa61)\r\n",
      "  Downloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\r\n",
      "Requirement already satisfied: torchaudio>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from fairseq==1.0.0a0+4acaa61) (2.4.0)\r\n",
      "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+4acaa61)\r\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.1.* in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.1->fairseq==1.0.0a0+4acaa61) (6.0.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.1->fairseq==1.0.0a0+4acaa61) (4.12.2)\r\n",
      "Collecting portalocker (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4acaa61)\r\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4acaa61) (0.9.0)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4acaa61) (0.4.6)\r\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4acaa61) (5.3.0)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi->fairseq==1.0.0a0+4acaa61) (2.22)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->fairseq==1.0.0a0+4acaa61) (3.15.1)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->fairseq==1.0.0a0+4acaa61) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->fairseq==1.0.0a0+4acaa61) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq==1.0.0a0+4acaa61) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->fairseq==1.0.0a0+4acaa61) (2024.6.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->fairseq==1.0.0a0+4acaa61) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->fairseq==1.0.0a0+4acaa61) (1.3.0)\r\n",
      "Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\r\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\r\n",
      "Building wheels for collected packages: fairseq, antlr4-python3-runtime\r\n",
      "  Building editable for fairseq (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for fairseq: filename=fairseq-1.0.0a0+4acaa61-0.editable-cp310-cp310-linux_x86_64.whl size=9403 sha256=42a05dd7ea6a59f456b795cfd70a2a8166b36638d1549b178ea89d531cf97607\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-n6qv_b4t/wheels/35/08/f6/d9fd34bf105b76f5aa33c01c514418ef0f846fcd6bc17c20ee\r\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141211 sha256=8e40bea38ed76813e4b03ad5716a6b3232be61ab6414b8f07c19968339f6243a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\r\n",
      "Successfully built fairseq antlr4-python3-runtime\r\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\r\n",
      "\u001b[0mInstalling collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, sacrebleu, hydra-core, fairseq\r\n",
      "Successfully installed antlr4-python3-runtime-4.8 bitarray-3.0.0 fairseq-1.0.0a0+4acaa61 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-3.0.0 sacrebleu-2.4.3\r\n"
     ]
    }
   ],
   "source": [
    "!cd fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1 && pip install --editable ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e736b37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T06:56:03.368968Z",
     "iopub.status.busy": "2024-11-27T06:56:03.368701Z",
     "iopub.status.idle": "2024-11-27T06:56:12.228919Z",
     "shell.execute_reply": "2024-11-27T06:56:12.227994Z"
    },
    "papermill": {
     "duration": 8.870211,
     "end_time": "2024-11-27T06:56:12.230996",
     "exception": false,
     "start_time": "2024-11-27T06:56:03.360785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa==0.9.1 (from -r requirements.txt (line 2))\r\n",
      "  Downloading librosa-0.9.1-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Collecting tensorboardX==2.5 (from -r requirements.txt (line 3))\r\n",
      "  Downloading tensorboardX-2.5-py2.py3-none-any.whl.metadata (5.2 kB)\r\n",
      "Requirement already satisfied: audioread>=2.1.5 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 2)) (3.0.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 2)) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 2)) (1.14.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 2)) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 2)) (1.4.2)\r\n",
      "Requirement already satisfied: decorator>=4.0.10 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 2)) (5.1.1)\r\n",
      "Collecting resampy>=0.2.2 (from librosa==0.9.1->-r requirements.txt (line 2))\r\n",
      "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Requirement already satisfied: numba>=0.45.1 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 2)) (0.60.0)\r\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 2)) (0.12.1)\r\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 2)) (1.8.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 2)) (21.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tensorboardX==2.5->-r requirements.txt (line 3)) (1.16.0)\r\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.10/site-packages (from tensorboardX==2.5->-r requirements.txt (line 3)) (3.20.3)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.45.1->librosa==0.9.1->-r requirements.txt (line 2)) (0.43.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->librosa==0.9.1->-r requirements.txt (line 2)) (3.1.2)\r\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.1->-r requirements.txt (line 2)) (3.11.0)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.1->-r requirements.txt (line 2)) (2.32.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->librosa==0.9.1->-r requirements.txt (line 2)) (3.5.0)\r\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.10.2->librosa==0.9.1->-r requirements.txt (line 2)) (1.16.0)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.1->-r requirements.txt (line 2)) (2.22)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1->-r requirements.txt (line 2)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1->-r requirements.txt (line 2)) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1->-r requirements.txt (line 2)) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1->-r requirements.txt (line 2)) (2024.8.30)\r\n",
      "Downloading librosa-0.9.1-py3-none-any.whl (213 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.1/213.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\r\n",
      "\u001b[0mInstalling collected packages: tensorboardX, resampy, librosa\r\n",
      "  Attempting uninstall: tensorboardX\r\n",
      "    Found existing installation: tensorboardX 2.6.2.2\r\n",
      "    Uninstalling tensorboardX-2.6.2.2:\r\n",
      "      Successfully uninstalled tensorboardX-2.6.2.2\r\n",
      "  Attempting uninstall: librosa\r\n",
      "    Found existing installation: librosa 0.10.2.post1\r\n",
      "    Uninstalling librosa-0.10.2.post1:\r\n",
      "      Successfully uninstalled librosa-0.10.2.post1\r\n",
      "Successfully installed librosa-0.9.1 resampy-0.4.3 tensorboardX-2.5\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6689b3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T06:56:12.247127Z",
     "iopub.status.busy": "2024-11-27T06:56:12.246511Z",
     "iopub.status.idle": "2024-11-27T06:56:43.827736Z",
     "shell.execute_reply": "2024-11-27T06:56:43.826881Z"
    },
    "papermill": {
     "duration": 31.591482,
     "end_time": "2024-11-27T06:56:43.829829",
     "exception": false,
     "start_time": "2024-11-27T06:56:12.238347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/working/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1')\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "from data_utils_SSL import genSpoof_list,Dataset_ASVspoof2019_train,Dataset_ASVspoof2021_eval\n",
    "from model import Model\n",
    "from tensorboardX import SummaryWriter\n",
    "from core_scripts.startup_config import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda5894",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T06:56:43.846215Z",
     "iopub.status.busy": "2024-11-27T06:56:43.845737Z",
     "iopub.status.idle": "2024-11-27T06:56:43.851937Z",
     "shell.execute_reply": "2024-11-27T06:56:43.851247Z"
    },
    "papermill": {
     "duration": 0.015706,
     "end_time": "2024-11-27T06:56:43.853397",
     "exception": false,
     "start_time": "2024-11-27T06:56:43.837691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 3,\n",
    "    'lr': 0.000001,\n",
    "    'weight_decay': 0.0001,\n",
    "    'loss': 'weighted_CCE',\n",
    "    'seed': 1234,  # Random seed for reproducibility\n",
    "    'model_path': None,  # Path to a model checkpoint\n",
    "    'comment': None,  # Description for the saved model\n",
    "    'track': 'LA',  # Dataset track; options are 'LA', 'PA', 'DF'\n",
    "    'eval_output': None,  # Path to save evaluation results\n",
    "    'eval': False,  # Boolean for evaluation mode\n",
    "    'is_eval': False,  # Boolean for evaluation dataset\n",
    "    'eval_part': 0,  # Part of evaluation to process\n",
    "    'cudnn_deterministic_toggle': False,  # Use deterministic CuDNN behavior\n",
    "    'cudnn_benchmark_toggle': True,  # Use CuDNN benchmark for faster runtime\n",
    "    \n",
    "    # RawBoost data augmentation options\n",
    "    'algo': 4,  # RawBoost algorithm selection\n",
    "\n",
    "    # LnL_convolutive_noise parameters\n",
    "    'nBands': 5,  # Number of notch filters\n",
    "    'minF': 20,  # Minimum center frequency of notch filter\n",
    "    'maxF': 8000,  # Maximum center frequency of notch filter\n",
    "    'minBW': 100,  # Minimum bandwidth of filter\n",
    "    'maxBW': 1000,  # Maximum bandwidth of filter\n",
    "    'minCoeff': 10,  # Minimum filter coefficients\n",
    "    'maxCoeff': 100,  # Maximum filter coefficients\n",
    "    'minG': 0,  # Minimum gain factor of linear component\n",
    "    'maxG': 0,  # Maximum gain factor of linear component\n",
    "    'minBiasLinNonLin': 5,  # Minimum gain difference between linear/non-linear components\n",
    "    'maxBiasLinNonLin': 20,  # Maximum gain difference between linear/non-linear components\n",
    "    'N_f': 5,  # Order of non-linearity (1 means only linear)\n",
    "\n",
    "    # ISD_additive_noise parameters\n",
    "    'P': 10,  # Max number of uniformly distributed samples in [%]\n",
    "    'g_sd': 2,  # Gain parameter for additive noise\n",
    "\n",
    "    # SSI_additive_noise parameters\n",
    "    'SNRmin': 10,  # Minimum SNR for colored noise\n",
    "    'SNRmax': 40  # Maximum SNR for colored noise\n",
    "}\n",
    "\n",
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "110b37eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T06:56:43.868435Z",
     "iopub.status.busy": "2024-11-27T06:56:43.868191Z",
     "iopub.status.idle": "2024-11-27T06:56:43.946834Z",
     "shell.execute_reply": "2024-11-27T06:56:43.945945Z"
    },
    "papermill": {
     "duration": 0.088116,
     "end_time": "2024-11-27T06:56:43.948562",
     "exception": false,
     "start_time": "2024-11-27T06:56:43.860446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cudnn_deterministic set to False\n",
      "cudnn_benchmark set to True\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "set_random_seed(args.seed, args)\n",
    "track = args.track\n",
    "assert track in ['LA', 'PA','DF'], 'Invalid track given'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1b6f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T06:56:43.964525Z",
     "iopub.status.busy": "2024-11-27T06:56:43.964238Z",
     "iopub.status.idle": "2024-11-27T06:56:43.982059Z",
     "shell.execute_reply": "2024-11-27T06:56:43.981408Z"
    },
    "papermill": {
     "duration": 0.027594,
     "end_time": "2024-11-27T06:56:43.983580",
     "exception": false,
     "start_time": "2024-11-27T06:56:43.955986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import fairseq\n",
    "\n",
    "class SSLModel(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(SSLModel, self).__init__()\n",
    "        \n",
    "        cp_path = '/kaggle/input/xlsr2-300m/xlsr2_300m.pt'\n",
    "        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\n",
    "        self.model = model[0].to(device)\n",
    "        self.device = device\n",
    "        self.out_dim = 1024\n",
    "\n",
    "    def extract_feat(self, input_data):\n",
    "        input_data = input_data.to(self.device)\n",
    "        input_tmp = input_data[:, :, 0] if input_data.ndim == 3 else input_data\n",
    "        emb = self.model(input_tmp, mask=False, features_only=True)['x']\n",
    "        \n",
    "        return emb\n",
    "\n",
    "\n",
    "class PSFAN_Backend(nn.Module):\n",
    "    def __init__(self, input_channels=128, num_classes=2):\n",
    "        super(PSFAN_Backend, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(input_channels, 128, kernel_size=3, dilation=1, padding=1)\n",
    "        self.conv1x1_1 = nn.Conv1d(128, 128, kernel_size=1)\n",
    "        self.conv3x3_1 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv1x1_2 = nn.Conv1d(128, 128, kernel_size=1)\n",
    "        self.attention1 = nn.Sigmoid()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(128, 128, kernel_size=3, dilation=2, padding=2)\n",
    "        self.conv1x1_3 = nn.Conv1d(128, 128, kernel_size=1)\n",
    "        self.conv3x3_2 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv1x1_4 = nn.Conv1d(128, 128, kernel_size=1)\n",
    "        self.attention2 = nn.Sigmoid()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, dilation=3, padding=3)\n",
    "        self.conv1x1_5 = nn.Conv1d(256, 256, kernel_size=1)\n",
    "        self.conv3x3_3 = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv1x1_6 = nn.Conv1d(256, 256, kernel_size=1)\n",
    "        self.attention3 = nn.Sigmoid()\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(256, 256, kernel_size=3, dilation=4, padding=4)\n",
    "        self.conv1x1_7 = nn.Conv1d(256, 256, kernel_size=1)\n",
    "        self.conv3x3_4 = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv1x1_8 = nn.Conv1d(256, 256, kernel_size=1)\n",
    "        self.attention4 = nn.Sigmoid()\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.gap1 = nn.AdaptiveAvgPool1d(1)\n",
    "        self.gap2 = nn.AdaptiveAvgPool1d(1)\n",
    "        self.gap3 = nn.AdaptiveAvgPool1d(1)\n",
    "        self.gap4 = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.fc_concat = nn.Linear(128 + 128 + 256 + 256, 16)\n",
    "        self.fc_out = nn.Linear(16, num_classes)\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x1_attention = self.attention1(self.conv1x1_1(self.conv3x3_1(self.conv1x1_2(x1))))\n",
    "        x1 = x1_attention * x1\n",
    "        x1 = self.pool1(x1)\n",
    "        x1_gap = self.gap1(x1).squeeze(-1)\n",
    "\n",
    "        x2 = self.conv2(x1)\n",
    "        x2_attention = self.attention2(self.conv1x1_3(self.conv3x3_2(self.conv1x1_4(x2))))\n",
    "        x2 = x2_attention * x2\n",
    "        x2 = self.pool2(x2)\n",
    "        x2_gap = self.gap2(x2).squeeze(-1)\n",
    "\n",
    "        x3 = self.conv3(x2)\n",
    "        x3_attention = self.attention3(self.conv1x1_5(self.conv3x3_3(self.conv1x1_6(x3))))\n",
    "        x3 = x3_attention * x3\n",
    "        x3 = self.pool3(x3)\n",
    "        x3_gap = self.gap3(x3).squeeze(-1)\n",
    "\n",
    "        x4 = self.conv4(x3)\n",
    "        x4_attention = self.attention4(self.conv1x1_7(self.conv3x3_4(self.conv1x1_8(x4))))\n",
    "        x4 = x4_attention * x4\n",
    "        x4 = self.pool4(x4)\n",
    "        x4_gap = self.gap4(x4).squeeze(-1)\n",
    "\n",
    "        x_concat = torch.cat([x1_gap, x2_gap, x3_gap, x4_gap], dim=1)\n",
    "\n",
    "        x = self.activation(self.fc_concat(x_concat))\n",
    "        output = self.fc_out(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args, device):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.ssl_model = SSLModel(self.device)\n",
    "        self.LL = nn.Linear(self.ssl_model.out_dim, 128).to(device)\n",
    "        self.backend = PSFAN_Backend(input_channels=128, num_classes=2).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x_ssl_feat = self.ssl_model.extract_feat(x)\n",
    "        x = self.LL(x_ssl_feat)\n",
    "        x = x.transpose(1, 2)\n",
    "        output = self.backend(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01bbcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T06:56:43.998913Z",
     "iopub.status.busy": "2024-11-27T06:56:43.998665Z",
     "iopub.status.idle": "2024-11-27T06:56:44.006001Z",
     "shell.execute_reply": "2024-11-27T06:56:44.005370Z"
    },
    "papermill": {
     "duration": 0.01662,
     "end_time": "2024-11-27T06:56:44.007421",
     "exception": false,
     "start_time": "2024-11-27T06:56:43.990801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import fairseq\n",
    "import glob\n",
    "\n",
    "def pad(x, max_len=64600):\n",
    "    x_len = x.shape[0]\n",
    "    if x_len >= max_len:\n",
    "        return x[:max_len]\n",
    "    num_repeats = int(max_len / x_len)+1\n",
    "    padded_x = np.tile(x, (1, num_repeats))[:, :max_len][0]\n",
    "    return padded_x\t\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, wav_files):\n",
    "        self.wav_files = wav_files\n",
    "        self.cut=64600\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wav_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path = self.wav_files[idx]\n",
    "        X, fs = librosa.load(wav_path, sr=16000)\n",
    "        X_pad = pad(X,self.cut)\n",
    "        x_inp = Tensor(X_pad)\n",
    "        \n",
    "        return x_inp, wav_path\n",
    "\n",
    "def inference(model, device, test_loader):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_paths in tqdm(test_loader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_out = model(batch_x)\n",
    "            \n",
    "            probs = torch.softmax(batch_out, dim=1)[:, 1]\n",
    "\n",
    "            results.extend(list(zip(batch_paths, probs.cpu().numpy())))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fcf3840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T06:56:44.022111Z",
     "iopub.status.busy": "2024-11-27T06:56:44.021868Z",
     "iopub.status.idle": "2024-11-27T06:56:44.025175Z",
     "shell.execute_reply": "2024-11-27T06:56:44.024566Z"
    },
    "papermill": {
     "duration": 0.01229,
     "end_time": "2024-11-27T06:56:44.026653",
     "exception": false,
     "start_time": "2024-11-27T06:56:44.014363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229cd9c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T06:56:44.041784Z",
     "iopub.status.busy": "2024-11-27T06:56:44.041294Z",
     "iopub.status.idle": "2024-11-27T08:52:09.390725Z",
     "shell.execute_reply": "2024-11-27T08:52:09.389630Z"
    },
    "papermill": {
     "duration": 6925.359123,
     "end_time": "2024-11-27T08:52:09.392826",
     "exception": false,
     "start_time": "2024-11-27T06:56:44.033703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/working/fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/fairseq/checkpoint_utils.py:313: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/tmp/ipykernel_23/1219766656.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ZALUPA_DICT = torch.load(MY_ZALUPA, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144693 wav files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9044 [00:00<?, ?it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "100%|██████████| 9044/9044 [1:54:50<00:00,  1.12s/it]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "100%|██████████| 9044/9044 [1:54:50<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to /kaggle/working/submission_6epochs_total.csv\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "MODEL = '/kaggle/input/w2v-airi/models/model_LA_weighted_CCE_3_16_1e-06/epoch_2.pth' # 0,1,2 from 1st notebook; 0,1 from 2nd notebook, soooo (0,1,2,0,1) - 5 epochs\n",
    "\n",
    "model = Model(args, device).to(device)\n",
    "\n",
    "DICT = torch.load(MODEL, map_location=device)\n",
    "model.load_state_dict(DICT)\n",
    "model.eval()\n",
    "\n",
    "test_wav_files = glob.glob('/kaggle/input/safe-speak-2024-audio-spoof-detection-hackathon/wavs/*.wav')\n",
    "print(f\"Found {len(test_wav_files)} wav files\")\n",
    "\n",
    "test_dataset = InferenceDataset(test_wav_files)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, num_workers=4, pin_memory=True)\n",
    "\n",
    "results = inference(model, device, test_loader)\n",
    "\n",
    "submission_data = []\n",
    "for filepath, score in results:\n",
    "    filename = os.path.basename(filepath)\n",
    "    submission_data.append({'ID': filename, 'score': score})\n",
    "\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_df.to_csv('submission_6epochs_total.csv', index=False)\n",
    "print(\"Submission file saved to /kaggle/working/submission_6epochs_total.csv\")\n",
    "\n",
    "del model, DICT, test_dataset, test_loader, results, submission_df\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10051766,
     "sourceId": 87433,
     "sourceType": "competition"
    },
    {
     "datasetId": 2286778,
     "sourceId": 3842332,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4827121,
     "sourceId": 8452149,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5952420,
     "sourceId": 9727510,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5968457,
     "sourceId": 9748923,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6056164,
     "sourceId": 9866535,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6058514,
     "sourceId": 9869755,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6170111,
     "sourceId": 10020341,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6170053,
     "sourceId": 10025816,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7056.927321,
   "end_time": "2024-11-27T08:52:46.917814",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-27T06:55:09.990493",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
